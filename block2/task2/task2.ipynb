{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd13deb4-521b-48a8-bcd4-db6ed995867c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mrjob in /usr/local/lib/python3.10/dist-packages (0.7.4)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from mrjob) (6.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57be2176-1022-4633-8663-e658f5c769e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (14.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.26.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb5054c7-254a-4a59-8d5a-99349289c8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task2.py\n"
     ]
    }
   ],
   "source": [
    "%%file task2.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRLinesCount(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper, combiner=self.combiner, reducer=self.reducer_cnt),\n",
    "            MRStep(reducer=self.reducer_max)\n",
    "        ]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        result = line.split('\\\"')\n",
    "        yield (result[3], result[-2])\n",
    "\n",
    "    def combiner(self, character, lines):\n",
    "        yield (character, max(lines, key=len))\n",
    "\n",
    "    def reducer_cnt(self, character, lines):\n",
    "        yield None, (character, max(lines, key=len))\n",
    "\n",
    "    def reducer_max(self, _, lines):\n",
    "        for line in sorted(lines, key=lambda x: len(x[1]), reverse=True):\n",
    "            yield line\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    MRLinesCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cce5238d-4b77-4e37-91ba-59d8f57fb5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['SW_EpisodeIV.txt', 'SW_EpisodeV.txt', 'SW_EpisodeVI.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1efc1a57-b4f3-48df-9ffb-20bb359bb99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/task2.root.20231206.051747.529599\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/task2.root.20231206.051747.529599/output\n",
      "Streaming final output from /tmp/task2.root.20231206.051747.529599/output...\n",
      "Removing temp directory /tmp/task2.root.20231206.051747.529599...\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/task2.root.20231206.051747.828516\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/task2.root.20231206.051747.828516/output\n",
      "Streaming final output from /tmp/task2.root.20231206.051747.828516/output...\n",
      "Removing temp directory /tmp/task2.root.20231206.051747.828516...\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/task2.root.20231206.051748.117743\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/task2.root.20231206.051748.117743/output\n",
      "Streaming final output from /tmp/task2.root.20231206.051748.117743/output...\n",
      "Removing temp directory /tmp/task2.root.20231206.051748.117743...\n"
     ]
    }
   ],
   "source": [
    "for file_name in file_names:\n",
    "    output_file = 'Result_'+ file_name\n",
    "    !python3 task2.py $file_name > $output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88255f7b-2d4f-4632-b4e2-72f4b3a30edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/task2.root.20231206.051903.270403\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/task2.root.20231206.051903.270403/output\n",
      "Streaming final output from /tmp/task2.root.20231206.051903.270403/output...\n",
      "Removing temp directory /tmp/task2.root.20231206.051903.270403...\n"
     ]
    }
   ],
   "source": [
    "!python3 task2.py SW_EpisodeIV.txt SW_EpisodeV.txt SW_EpisodeVI.txt > Result_EpisodeAll.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b46be4e-e83c-49ae-bace-1c79c1138774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/task2.root.20231206.052704.953780\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/task2.root.20231206.052704.953780/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/task2.root.20231206.052704.953780/files/\n",
      "Running step 1 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar7448963435017902807/] [] /tmp/streamjob1205225726761750731.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.24.0.5:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.24.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701811005997_0025\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701811005997_0025\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701811005997_0025\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701811005997_0025/\n",
      "  Running job: job_1701811005997_0025\n",
      "  Job job_1701811005997_0025 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701811005997_0025 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/task2.root.20231206.052704.953780/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=82374\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9094\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=10546\n",
      "\t\tFILE: Number of bytes written=870436\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=82552\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=9094\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3465216\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1413120\n",
      "\t\tTotal time spent by all map tasks (ms)=3384\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6768\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1380\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2760\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3384\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1380\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=950\n",
      "\t\tCombine input records=1011\n",
      "\t\tCombine output records=73\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=105\n",
      "\t\tInput split bytes=178\n",
      "\t\tMap input records=1011\n",
      "\t\tMap output bytes=72334\n",
      "\t\tMap output materialized bytes=10552\n",
      "\t\tMap output records=1011\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=256032768\n",
      "\t\tPeak Map Virtual memory (bytes)=2472767488\n",
      "\t\tPeak Reduce Physical memory (bytes)=214315008\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2481090560\n",
      "\t\tPhysical memory (bytes) snapshot=725913600\n",
      "\t\tReduce input groups=61\n",
      "\t\tReduce input records=73\n",
      "\t\tReduce output records=61\n",
      "\t\tReduce shuffle bytes=10552\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=146\n",
      "\t\tTotal committed heap usage (bytes)=584056832\n",
      "\t\tVirtual memory (bytes) snapshot=7426064384\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar1233374137651615952/] [] /tmp/streamjob4941333300731980788.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.24.0.5:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.24.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701811005997_0026\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701811005997_0026\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701811005997_0026\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701811005997_0026/\n",
      "  Running job: job_1701811005997_0026\n",
      "  Job job_1701811005997_0026 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701811005997_0026 completed successfully\n",
      "  Output directory: hdfs:///Hadoop_Result_SW_EpisodeIV.txt\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=13190\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8606\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=9286\n",
      "\t\tFILE: Number of bytes written=866431\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=13488\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=8606\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3177472\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1424384\n",
      "\t\tTotal time spent by all map tasks (ms)=3103\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6206\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1391\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2782\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3103\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1391\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=900\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=103\n",
      "\t\tInput split bytes=298\n",
      "\t\tMap input records=61\n",
      "\t\tMap output bytes=9126\n",
      "\t\tMap output materialized bytes=9292\n",
      "\t\tMap output records=61\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=302886912\n",
      "\t\tPeak Map Virtual memory (bytes)=2473451520\n",
      "\t\tPeak Reduce Physical memory (bytes)=216391680\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2480226304\n",
      "\t\tPhysical memory (bytes) snapshot=821800960\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=61\n",
      "\t\tReduce output records=61\n",
      "\t\tReduce shuffle bytes=9292\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=122\n",
      "\t\tTotal committed heap usage (bytes)=728236032\n",
      "\t\tVirtual memory (bytes) snapshot=7425757184\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///Hadoop_Result_SW_EpisodeIV.txt\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/task2.root.20231206.052704.953780...\n",
      "Removing temp directory /tmp/task2.root.20231206.052704.953780...\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/task2.root.20231206.052746.987806\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/task2.root.20231206.052746.987806/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/task2.root.20231206.052746.987806/files/\n",
      "Running step 1 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar2608769639117748192/] [] /tmp/streamjob7278148819199135223.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.24.0.5:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.24.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701811005997_0027\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701811005997_0027\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701811005997_0027\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701811005997_0027/\n",
      "  Running job: job_1701811005997_0027\n",
      "  Job job_1701811005997_0027 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701811005997_0027 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/task2.root.20231206.052746.987806/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=59583\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5603\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6692\n",
      "\t\tFILE: Number of bytes written=862725\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=59759\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=5603\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3869696\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1394688\n",
      "\t\tTotal time spent by all map tasks (ms)=3779\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7558\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1362\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2724\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3779\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1362\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=990\n",
      "\t\tCombine input records=840\n",
      "\t\tCombine output records=60\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=107\n",
      "\t\tInput split bytes=176\n",
      "\t\tMap input records=840\n",
      "\t\tMap output bytes=50578\n",
      "\t\tMap output materialized bytes=6698\n",
      "\t\tMap output records=840\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=305147904\n",
      "\t\tPeak Map Virtual memory (bytes)=2472587264\n",
      "\t\tPeak Reduce Physical memory (bytes)=244174848\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2480619520\n",
      "\t\tPhysical memory (bytes) snapshot=853098496\n",
      "\t\tReduce input groups=50\n",
      "\t\tReduce input records=60\n",
      "\t\tReduce output records=50\n",
      "\t\tReduce shuffle bytes=6698\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=120\n",
      "\t\tTotal committed heap usage (bytes)=740818944\n",
      "\t\tVirtual memory (bytes) snapshot=7425363968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar6564747507359930740/] [] /tmp/streamjob8448203877431901616.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.24.0.5:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.24.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701811005997_0028\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701811005997_0028\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701811005997_0028\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701811005997_0028/\n",
      "  Running job: job_1701811005997_0028\n",
      "  Job job_1701811005997_0028 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701811005997_0028 completed successfully\n",
      "  Output directory: hdfs:///Hadoop_Result_SW_EpisodeV.txt\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8405\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5203\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5744\n",
      "\t\tFILE: Number of bytes written=859344\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8703\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=5203\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3106816\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1467392\n",
      "\t\tTotal time spent by all map tasks (ms)=3034\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6068\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1433\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2866\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3034\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1433\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=880\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=110\n",
      "\t\tInput split bytes=298\n",
      "\t\tMap input records=50\n",
      "\t\tMap output bytes=5620\n",
      "\t\tMap output materialized bytes=5750\n",
      "\t\tMap output records=50\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=255959040\n",
      "\t\tPeak Map Virtual memory (bytes)=2472312832\n",
      "\t\tPeak Reduce Physical memory (bytes)=216666112\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2479542272\n",
      "\t\tPhysical memory (bytes) snapshot=726110208\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=50\n",
      "\t\tReduce output records=50\n",
      "\t\tReduce shuffle bytes=5750\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=100\n",
      "\t\tTotal committed heap usage (bytes)=581435392\n",
      "\t\tVirtual memory (bytes) snapshot=7423873024\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///Hadoop_Result_SW_EpisodeV.txt\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/task2.root.20231206.052746.987806...\n",
      "Removing temp directory /tmp/task2.root.20231206.052746.987806...\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/task2.root.20231206.052829.058351\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/task2.root.20231206.052829.058351/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/task2.root.20231206.052829.058351/files/\n",
      "Running step 1 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar1674833870714503226/] [] /tmp/streamjob4339556007091012827.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.24.0.5:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.24.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701811005997_0029\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701811005997_0029\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701811005997_0029\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701811005997_0029/\n",
      "  Running job: job_1701811005997_0029\n",
      "  Job job_1701811005997_0029 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701811005997_0029 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/task2.root.20231206.052829.058351/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=52272\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=7021\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7950\n",
      "\t\tFILE: Number of bytes written=865244\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=52450\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=7021\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3536896\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1371136\n",
      "\t\tTotal time spent by all map tasks (ms)=3454\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6908\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1339\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2678\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3454\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1339\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1010\n",
      "\t\tCombine input records=675\n",
      "\t\tCombine output records=63\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=111\n",
      "\t\tInput split bytes=178\n",
      "\t\tMap input records=675\n",
      "\t\tMap output bytes=44262\n",
      "\t\tMap output materialized bytes=7956\n",
      "\t\tMap output records=675\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=255504384\n",
      "\t\tPeak Map Virtual memory (bytes)=2471751680\n",
      "\t\tPeak Reduce Physical memory (bytes)=245489664\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2479468544\n",
      "\t\tPhysical memory (bytes) snapshot=755666944\n",
      "\t\tReduce input groups=54\n",
      "\t\tReduce input records=63\n",
      "\t\tReduce output records=54\n",
      "\t\tReduce shuffle bytes=7956\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=126\n",
      "\t\tTotal committed heap usage (bytes)=585105408\n",
      "\t\tVirtual memory (bytes) snapshot=7422894080\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar5156721317420023476/] [] /tmp/streamjob2689891000156398503.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.24.0.5:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.24.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701811005997_0030\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701811005997_0030\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701811005997_0030\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701811005997_0030/\n",
      "  Running job: job_1701811005997_0030\n",
      "  Job job_1701811005997_0030 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701811005997_0030 completed successfully\n",
      "  Output directory: hdfs:///Hadoop_Result_SW_EpisodeVI.txt\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=10532\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6589\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7179\n",
      "\t\tFILE: Number of bytes written=862217\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=10830\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=6589\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3116032\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1366016\n",
      "\t\tTotal time spent by all map tasks (ms)=3043\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6086\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1334\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2668\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3043\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1334\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=820\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=132\n",
      "\t\tInput split bytes=298\n",
      "\t\tMap input records=54\n",
      "\t\tMap output bytes=7043\n",
      "\t\tMap output materialized bytes=7185\n",
      "\t\tMap output records=54\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=254627840\n",
      "\t\tPeak Map Virtual memory (bytes)=2472005632\n",
      "\t\tPeak Reduce Physical memory (bytes)=243519488\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2480631808\n",
      "\t\tPhysical memory (bytes) snapshot=752271360\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=54\n",
      "\t\tReduce output records=54\n",
      "\t\tReduce shuffle bytes=7185\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=108\n",
      "\t\tTotal committed heap usage (bytes)=585629696\n",
      "\t\tVirtual memory (bytes) snapshot=7424602112\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///Hadoop_Result_SW_EpisodeVI.txt\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/task2.root.20231206.052829.058351...\n",
      "Removing temp directory /tmp/task2.root.20231206.052829.058351...\n"
     ]
    }
   ],
   "source": [
    "for file_name in file_names:\n",
    "    input_file = 'hdfs://namenode:8020/' + file_name\n",
    "    output_file = '/Hadoop_Result_'+ file_name\n",
    "    !python3 task2.py -r hadoop $input_file --output $output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb00192a-872b-472e-a40b-701b0109aa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-06 08:30:14 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-12-06 08:30:15 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-12-06 08:30:17 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "for file_name in file_names:\n",
    "    input_file = 'hdfs://namenode:8020/Hadoop_Result_' + file_name + '/part-00000'\n",
    "    output_file = 'Hadoop_Result_'+ file_name\n",
    "    !hadoop fs -get $input_file $output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b1d9418-776d-4965-9b6a-d238aa2a1db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/task2.root.20231206.053236.940209\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/task2.root.20231206.053236.940209/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/task2.root.20231206.053236.940209/files/\n",
      "Running step 1 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar7491208522132495394/] [] /tmp/streamjob3901650638494881949.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.24.0.5:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.24.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701811005997_0032\n",
      "  Total input files to process : 3\n",
      "  number of splits:3\n",
      "  Submitting tokens for job: job_1701811005997_0032\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701811005997_0032\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701811005997_0032/\n",
      "  Running job: job_1701811005997_0032\n",
      "  Job job_1701811005997_0032 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701811005997_0032 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/task2.root.20231206.053236.940209/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=181941\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=17133\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=20864\n",
      "\t\tFILE: Number of bytes written=1174509\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=182207\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=17133\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6969344\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1373184\n",
      "\t\tTotal time spent by all map tasks (ms)=6806\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13612\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1341\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2682\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6806\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1341\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1570\n",
      "\t\tCombine input records=2526\n",
      "\t\tCombine output records=165\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=192\n",
      "\t\tInput split bytes=266\n",
      "\t\tMap input records=2526\n",
      "\t\tMap output bytes=167174\n",
      "\t\tMap output materialized bytes=20876\n",
      "\t\tMap output records=2526\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tPeak Map Physical memory (bytes)=305475584\n",
      "\t\tPeak Map Virtual memory (bytes)=2472771584\n",
      "\t\tPeak Reduce Physical memory (bytes)=243998720\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2481217536\n",
      "\t\tPhysical memory (bytes) snapshot=1109880832\n",
      "\t\tReduce input groups=130\n",
      "\t\tReduce input records=165\n",
      "\t\tReduce output records=130\n",
      "\t\tReduce shuffle bytes=20876\n",
      "\t\tShuffled Maps =3\n",
      "\t\tSpilled Records=330\n",
      "\t\tTotal committed heap usage (bytes)=924319744\n",
      "\t\tVirtual memory (bytes) snapshot=9898442752\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar6823617257897273404/] [] /tmp/streamjob8088783517129889431.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.24.0.5:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.24.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701811005997_0033\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701811005997_0033\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701811005997_0033\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701811005997_0033/\n",
      "  Running job: job_1701811005997_0033\n",
      "  Job job_1701811005997_0033 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701811005997_0033 completed successfully\n",
      "  Output directory: hdfs:///Hadoop_Result_EpisodeAll.txt\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=21229\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=16093\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=17501\n",
      "\t\tFILE: Number of bytes written=882855\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=21527\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=16093\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3133440\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1426432\n",
      "\t\tTotal time spent by all map tasks (ms)=3060\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6120\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1393\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2786\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3060\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1393\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=860\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=113\n",
      "\t\tInput split bytes=298\n",
      "\t\tMap input records=130\n",
      "\t\tMap output bytes=17184\n",
      "\t\tMap output materialized bytes=17507\n",
      "\t\tMap output records=130\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=255528960\n",
      "\t\tPeak Map Virtual memory (bytes)=2473476096\n",
      "\t\tPeak Reduce Physical memory (bytes)=244572160\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2480504832\n",
      "\t\tPhysical memory (bytes) snapshot=755527680\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=130\n",
      "\t\tReduce output records=130\n",
      "\t\tReduce shuffle bytes=17507\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=260\n",
      "\t\tTotal committed heap usage (bytes)=581435392\n",
      "\t\tVirtual memory (bytes) snapshot=7427149824\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///Hadoop_Result_EpisodeAll.txt\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/task2.root.20231206.053236.940209...\n",
      "Removing temp directory /tmp/task2.root.20231206.053236.940209...\n"
     ]
    }
   ],
   "source": [
    "!python3 task2.py -r hadoop hdfs://namenode:8020/SW_EpisodeIV.txt hdfs://namenode:8020/SW_EpisodeV.txt hdfs://namenode:8020/SW_EpisodeVI.txt --output /Hadoop_Result_EpisodeAll.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12912c4c-a5bb-42e1-9fa5-e732f7b4ae08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-06 08:33:52 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -get hdfs://namenode:8020/Hadoop_Result_EpisodeAll.txt/part-00000 Hadoop_Result_EpisodeAll.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8b5df09-d461-4586-8e41-230333a1079c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-06 08:32:31 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted /Hadoop_Result_EpisodeAll.txt\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r /Hadoop_Result_EpisodeAll.txt\n",
    "# !hadoop fs -rm -r /Hadoop_Result_SW_EpisodeV.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b5c64-d92e-4f75-82d0-fe913ec4cbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
